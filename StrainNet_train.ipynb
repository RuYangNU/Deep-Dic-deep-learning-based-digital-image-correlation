{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "from torch.nn import init\n",
    "\n",
    "from torchvision.models.resnet import BasicBlock, ResNet\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from torchvision import models, transforms\n",
    "import torch.utils.data as data_utils\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "def default_loader(path):\n",
    "    return Image.open(path)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv(in_planes, out_planes, kernel_size=3, stride=1, dilation=1, bias=False, transposed=False):\n",
    "    if transposed:\n",
    "        layer = nn.ConvTranspose2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=1, output_padding=1,\n",
    "                                   dilation=dilation, bias=bias)\n",
    "    else:\n",
    "        padding = (kernel_size + 2 * (dilation - 1)) // 2\n",
    "        layer = nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, bias=bias)\n",
    "    if bias:\n",
    "        init.constant(layer.bias, 0)\n",
    "    return layer\n",
    "\n",
    "# Returns 2D batch normalisation layer\n",
    "def bn(planes):\n",
    "    layer = nn.BatchNorm2d(planes)\n",
    "    # Use mean 0, standard deviation 1 init\n",
    "    init.constant(layer.weight, 1)\n",
    "    init.constant(layer.bias, 0)\n",
    "    return layer\n",
    "\n",
    "\n",
    "class FeatureResNet(ResNet):\n",
    "    def __init__(self):\n",
    "        super().__init__(BasicBlock, [3, 4, 6, 3], 1000)\n",
    "        self.conv_f = conv(2,64, kernel_size=7,stride = 1)\n",
    "        #self.bn_f = bn(64)\n",
    "        self.ReLu_1 = nn.ReLU(inplace=True)\n",
    "        self.conv_pre = conv(512, 1024, stride=2, transposed=False)\n",
    "        self.bn_pre = bn(1024)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.conv_f(x)\n",
    "        x = self.bn1(x1)\n",
    "        x = self.relu(x)\n",
    "        x2 = self.maxpool(x)\n",
    "        x = self.layer1(x2)\n",
    "        x3 = self.layer2(x)\n",
    "        x4 = self.layer3(x3)\n",
    "        x5 = self.layer4(x4)\n",
    "       \n",
    "        return x1, x2, x3, x4, x5\n",
    "    \n",
    "\n",
    "\n",
    "class SegResNet(nn.Module):\n",
    "    def __init__(self, num_classes, pretrained_net):\n",
    "        super().__init__()\n",
    "        self.pretrained_net = pretrained_net\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv3_2 = conv(512, 256, stride=1, transposed=False)\n",
    "        self.bn3_2 = bn(256)\n",
    "        self.conv5 = conv(256, 256, stride=2, transposed=True)\n",
    "        self.bn5 = bn(256)\n",
    "        self.conv5_2 = conv(256, 256, stride=1, transposed=False)\n",
    "        self.bn5_2 = bn(256)\n",
    "        self.conv6 = conv(256, 128, stride=2, transposed=True)\n",
    "        self.bn6 = bn(128)\n",
    "        self.conv7 = conv(128, 64, stride=2, transposed=True)\n",
    "        self.bn7 = bn(64)\n",
    "        self.conv8 = conv(64, 64, stride=2, transposed=True)\n",
    "        self.bn8 = bn(64)\n",
    "        self.conv9 = conv(64, 32, stride=1, transposed=False)\n",
    "        self.bn9 = bn(32)\n",
    "        self.conv10 = conv(32, num_classes,stride=1, kernel_size=7)\n",
    "        init.constant(self.conv10.weight, 0)  # Zero init\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1, x2, x3, x4, x5 = self.pretrained_net(x)\n",
    "        \n",
    "        x = self.relu(self.bn3_2(self.conv3_2(x5)))\n",
    "        \n",
    "        \n",
    "        x = self.relu(self.bn5(self.conv5(x)))\n",
    "        x = self.relu(self.bn5_2(self.conv5_2(x)))\n",
    "        x = self.relu(self.bn6(self.conv6(x )))\n",
    "        x = self.relu(self.bn7(self.conv7(x+x3 )))\n",
    "        x = self.relu(self.bn8(self.conv8(x+x2 )))\n",
    "        x = self.relu(self.bn9(self.conv9(x)))\n",
    "        x = self.conv10(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnet = FeatureResNet()\n",
    "fcn = SegResNet(4,fnet)\n",
    "fcn = fcn.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = ''\n",
    "filename = \"validation_dataset.txt\"\n",
    "mynumbers = []\n",
    "with open(filename) as f:\n",
    "    for line in f:\n",
    "        item = line.strip().split('\\n')\n",
    "        for subitem in item:\n",
    "            mynumbers.append(subitem)\n",
    "            \n",
    "test_set = []\n",
    "for i in range(4000):\n",
    "    test_set.append((dataset_path+'/imgs3/train_image_'+str(z+1)+'_1.png',\n",
    "                       dataset_path+'/imgs3/train_image_'+str(z+1)+'_2.png',\n",
    "                       dataset_path+'/gt3/train_image_'+str(z+1)+'.mat'))\n",
    "\n",
    "dataset_path = ''\n",
    "filename = \"train_dataset.txt\"\n",
    "mynumbers = []\n",
    "with open(filename) as f:\n",
    "    for line in f:\n",
    "        item = line.strip().split('\\n')\n",
    "        for subitem in item:\n",
    "            mynumbers.append(subitem)\n",
    "            \n",
    "test_set = []\n",
    "for i in range(36000):\n",
    "    train_set.append((dataset_path+'/imgs3/train_image_'+str(z+1)+'_1.png',\n",
    "                       dataset_path+'/imgs3/train_image_'+str(z+1)+'_2.png',\n",
    "                       dataset_path+'/gt3/train_image_'+str(z+1)+'.mat'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "#light_index_2 = [2,7,15,8,4,22,13,57,54,40,91,21,29,84,71,25,28,51,67,62,34,46,93,87]\n",
    "class MyDataset(data_utils.Dataset):\n",
    "    def __init__(self, dataset, transform=None, target_transform=None, loader=default_loader):\n",
    "       \n",
    "        self.imgs = dataset\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.loader = loader\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        label_x, label_y, label_z = self.imgs[index]\n",
    "        img1 = self.loader(label_x)\n",
    "        img_1 = ToTensor()(img1.resize((128,128)))\n",
    "        img2 = self.loader(label_y)\n",
    "        img_2 = ToTensor()(img2.resize((128,128)))\n",
    "        imgs = torch.cat((img_1, img_2), 0)\n",
    "        try:\n",
    "            gt = sio.loadmat(label_z)['Disp_field_1'].astype(float)\n",
    "            \n",
    "        except KeyError:\n",
    "            gt = sio.loadmat(label_z)['Disp_field_2'].astype(float)\n",
    "           \n",
    "        gt = np.asarray(gt)\n",
    "        gt = gt*100\n",
    "        [dudx, dudy]= np.gradient(gt[:,:,0])\n",
    "        [dvdx, dvdy]= np.gradient(gt[:,:,1])\n",
    "        \n",
    "        f = interpolate.interp2d(x, y, dudx, kind='cubic')\n",
    "        dudx_ = f(xnew, ynew)\n",
    "        f = interpolate.interp2d(x, y, dudy, kind='cubic')\n",
    "        dudy_ = f(xnew, ynew)\n",
    "        f = interpolate.interp2d(x, y, dvdx, kind='cubic')\n",
    "        dvdx_ = f(xnew, ynew)\n",
    "        f = interpolate.interp2d(x, y, dvdy, kind='cubic')\n",
    "        dvdy_ = f(xnew, ynew)\n",
    "        st = np.stack([dudx_, dudy_, dvdx_, dvdy_], axis=0)\n",
    "                #st = np.stack([dudx, dudy, dvdx, dvdy], axis=0)\n",
    "        \n",
    "        \n",
    "        return imgs,st\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 100              # train the training data n times, to save time, we just train 1 epoch\n",
    "BATCH_SIZE = 12\n",
    "print('BATCH_SIZE = ',BATCH_SIZE)\n",
    "LR = 0.001              # learning rate\n",
    "#root = './gdrive_northwestern/My Drive/dl_encoder/data/orig/orig'\n",
    "NUM_WORKERS = 0\n",
    "\n",
    "optimizer = torch.optim.Adam(fcn.parameters(), lr=LR)   # optimize all cnn parameters\n",
    "#optimizer = torch.optim.SGD(cnn.parameters(), lr=LR, momentum=0.9)   # optimize all cnn parameters\n",
    "loss_func = nn.MSELoss()\n",
    "\n",
    "\n",
    "train_data=MyDataset(dataset=train_set)\n",
    "train_loader = data_utils.DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "\n",
    "test_data=MyDataset(dataset=test_set)\n",
    "test_loader = data_utils.DataLoader(dataset=test_data, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "dataString = datetime.strftime(datetime.now(), '%Y_%m_%d_%H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_result = ''\n",
    "os.mkdir(root_result)\n",
    "model_result = root_result+'model/'\n",
    "log_result = root_result+'log/'\n",
    "os.mkdir(model_result)\n",
    "os.mkdir(log_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileOut=open(log_result+'log'+dataString,'a')\n",
    "fileOut.write(dataString+'Epoch:   Step:    Loss:        Val_Accu :\\n')\n",
    "fileOut.close()\n",
    "fileOut2 = open(log_result+'validation'+dataString, 'a')\n",
    "fileOut2.write('kernal_size of conv_f is 2')\n",
    "fileOut2.write(dataString+'Epoch:    loss:')\n",
    "\n",
    "optimizer = torch.optim.Adam(fcn.parameters(), lr=0.00001)   # optimize all cnn parameters\n",
    "\n",
    "fcn.load_state_dict(torch.load(model_result + 'param_all_strain_99_1124'))\n",
    "for epoch in range(EPOCH):\n",
    "    fcn.train()\n",
    "    for step, (img,gt) in enumerate(train_loader):   # gives batch data, normalize x when iterate train_loader\n",
    "        \n",
    "        img = Variable(img).cuda()\n",
    "        gt=gt.float()\n",
    "        gt = Variable(gt).cuda()\n",
    "        output = fcn(img)               # cnn output\n",
    "        loss = loss_func(output, gt)    # loss\n",
    "        optimizer.zero_grad()           # clear gradients for this training step\n",
    "        loss.backward()                 # backpropagation, compute gradients\n",
    "        optimizer.step()                # apply gradients\n",
    "        print(epoch,  step, loss.data.item())\n",
    "        fileOut=open(log_result+'log'+dataString,'a')\n",
    "        fileOut.write(str(epoch)+'   '+str(step)+'   '+str(loss.data.item())+'\\n')\n",
    "        fileOut.close()\n",
    "    if epoch%10 == 9:\n",
    "        PATH = model_result + 'param_all_strain2_' + str(epoch) + '_' + str(step)\n",
    "        torch.save(fcn.state_dict(), PATH)\n",
    "        print('finished saving checkpoints')\n",
    "     \n",
    "    LOSS_VALIDATION = 0\n",
    "    fcn.eval()\n",
    "    with torch.no_grad():\n",
    "        for step, (img,gt) in enumerate(test_loader):\n",
    "\n",
    "            img = Variable(img).cuda()\n",
    "            gt=gt.unsqueeze(1)# batch x\n",
    "            gt = Variable(gt).cuda()\n",
    "            output = fcn(img) \n",
    "            LOSS_VALIDATION += loss_func(output, gt)\n",
    "        LOSS_VALIDATION = LOSS_VALIDATION/step\n",
    "        fileOut2 = open(log_result+'validation'+dataString, 'a')\n",
    "        fileOut2.write(str(epoch)+'   '+str(step)+'   '+str(LOSS_VALIDATION.data.item())+'\\n')\n",
    "        fileOut2.close()\n",
    "        print('validation error epoch  '+str(epoch)+':    '+str(LOSS_VALIDATION)+'\\n'+str(step))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
